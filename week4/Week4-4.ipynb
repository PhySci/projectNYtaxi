{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model, model_selection, preprocessing\n",
    "\n",
    "import datetime\n",
    "from itertools import product\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def regression(res, alpha = 0.1, plot = False,verbose = False, searchBestFit = False):\n",
    "    X = res.drop('trip_count',axis = 1)        \n",
    "    y = res.loc[:,'trip_count'];\n",
    "    \n",
    "    if searchBestFit:\n",
    "        # создать словарь параметров\n",
    "        param_grid = {'alpha': [x for x in np.linspace(1,100,10)]} #,'l1_ratio': [0.1, 0.25, 0.4, 0.5, 0.6, 0.75, 1]}\n",
    "        # создать кросс-валидацию для временных рядов\n",
    "        tscv = model_selection.TimeSeriesSplit()\n",
    "        \n",
    "        # запустить поиск оптимальных параметров\n",
    "        regressor = linear_model.Ridge()\n",
    "        clf = model_selection.GridSearchCV(regressor, param_grid, n_jobs=4, cv=tscv, verbose=1)\n",
    "        clf.fit(X,y)\n",
    "        regressor = clf.best_estimator_        \n",
    "        print 'Best params is', clf.best_params_ \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        regressor = linear_model.Lasso(alpha = alpha, max_iter = 1e5,fit_intercept = True,random_state = 0);\n",
    "        regressor.fit(X,y)\n",
    "        \n",
    "    y_pr = pd.Series(data = regressor.predict(X), index = res.index)\n",
    "    R = regressor.score(X,y);\n",
    "    print 'R factor is ', R\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize = [15,10])\n",
    "        plt.subplot(211)\n",
    "        plt.plot(y)\n",
    "        plt.plot(y_pr)\n",
    "        plt.legend(['Original data','Predicted'])\n",
    "\n",
    "        plt.subplot(212)\n",
    "        plt.plot(y-y_pr)\n",
    "        plt.legend(['Residuals'])\n",
    "        \n",
    "    return [y_pr, y-y_pr, regressor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getRegressor(regressor, start_date = '2016-05-15 00:00:00', end_date = '2016-05-20 23:00:00'):\n",
    "    predictionStart = datetime.datetime.strptime(start_date,'%Y-%m-%d %H:%M:%S')\n",
    "    predictionEnd = datetime.datetime.strptime(end_date,'%Y-%m-%d %H:%M:%S')\n",
    "    date_index = pd.date_range(predictionStart, predictionEnd, freq='H')\n",
    "   \n",
    "    #какой-то пипец. Должен быть способ сделать это проще.\n",
    "    features = date_index.to_series().to_frame()\n",
    "    features = addFeatures(features,verbose = True)\n",
    "    features = features.drop(0,axis = 1)\n",
    "    exog = regressor.predict(features)\n",
    "    return pd.Series(exog,index = date_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addFeatures(res, Kw = 6, Ka = 3,verbose = False):    \n",
    "    # add linear feature\n",
    "    res = res.assign(hours = (res.index - datetime.datetime(2014,1,1,0,0,0))/np.timedelta64(1, 'h'))\n",
    "    \n",
    "    # добавляем гармонические фичи\n",
    "    for ind in range(1,Kw+1):\n",
    "        res['weekCos'+str(ind)]= np.cos(np.pi*res.hours*ind/168);\n",
    "        res['weekSin'+str(ind)]= np.sin(np.pi*res.hours*ind/168);\n",
    "    for ind in range(1,Ka+1):\n",
    "        res['yearCos'+str(ind)]= np.cos(2*np.pi*res.hours*ind/8766);\n",
    "        res['yearSin'+str(ind)]= np.sin(2*np.pi*res.hours*ind/8766);\n",
    "        \n",
    "    # добавляем dummy variables для дней недели\n",
    "    lbDays = preprocessing.LabelBinarizer()\n",
    "    lbDays.fit(list(np.arange(6)))\n",
    "    DoW = pd.DataFrame(lbDays.transform(res.index.dayofweek),columns = ['DayOfWeek_'+str(x) for x in np.arange(6)],\n",
    "                       index = res.index)      \n",
    "    res = res.merge(DoW,left_index=True,right_index=True)\n",
    " \n",
    "    # добавляем dummy variables для месяца\n",
    "    lbMonths = preprocessing.LabelBinarizer()\n",
    "    lbMonths.fit(list(np.arange(12)))\n",
    "    Months = pd.DataFrame(lbMonths.transform(res.index.month),columns = ['Month_'+str(x) for x in np.arange(12)],index = res.index)      \n",
    "    res = res.merge(Months,left_index=True,right_index=True);\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findHyperParams(ts,pList = []):\n",
    "    \n",
    "    if (len(pList) ==0):\n",
    "        # create list of parameters\n",
    "        ps = range(2, 7)\n",
    "        d  = 1\n",
    "        qs = range(2, 7)\n",
    "\n",
    "        Ps = range(1, 3)\n",
    "        D  = 1\n",
    "        Qs = range(1, 3)\n",
    "\n",
    "        pList = list(product(ps, qs, Ps, Qs))\n",
    "    \n",
    "    results = []\n",
    "    best_aic = float(\"inf\")\n",
    "    \n",
    "    # add features\n",
    "    ts = addFeatures(ts, Kw = 6, Ka = 3)\n",
    "    # regression\n",
    "    [s, r, lasso] = regression(ts,verbose = True, searchBestFit = True)\n",
    "    \n",
    "    # loop over parameters' list\n",
    "    for param in pList:\n",
    "        #try except нужен, потому что на некоторых наборах параметров модель не обучается\n",
    "        print('Parameters:', param)\n",
    "        try:\n",
    "            mSARIMA=sm.tsa.statespace.SARIMAX(ts.loc[:,'trip_count'], order=[param[0], 1, param[1]],\n",
    "                                          seasonal_order=(param[2], 1, param[3], 24),exog = r).fit(disp=1);\n",
    "        #выводим параметры, на которых модель не обучается и переходим к следующему набору\n",
    "        except Exception as inst:\n",
    "            print inst           # __str__ allows args to be printed directly\n",
    "            continue\n",
    "        else:     \n",
    "            aic = mSARIMA.aic\n",
    "            print('AIC',aic) \n",
    "            #сохраняем лучшую модель, aic, параметры\n",
    "            if aic < best_aic:\n",
    "                best_model = mSARIMA\n",
    "                best_aic = aic\n",
    "                best_param = param\n",
    "            results.append([param, mSARIMA.aic])\n",
    "    return [best_aic,best_param, best_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# id нужных регионов\n",
    "regsDf = pd.read_csv('../crowdRegs.csv',names=['id','regId']);  \n",
    "\n",
    "# временные ряды для этих регионов\n",
    "df = pd.read_pickle('../loadData/crowdRegs3.pcl')\n",
    "df.columns = regsDf.regId.values.astype('str')\n",
    "\n",
    "# словарь с группировкой рядов\n",
    "tsGroups = np.load('tsGroups.npy').item()\n",
    "\n",
    "# словарь с оптимальными параметрами для каждой группы\n",
    "paramsGroups = np.load('paramsGroups.npy').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Логика скрипта:*\n",
    "<ol>\n",
    "<li> Выбираем одну группу\n",
    "<li> В группе выбираем один ряд\n",
    "<li> По номеру группы подгружаем оптимальные параметры\n",
    "<li> Обучаем регрессор\n",
    "<li> Обучаем SARIMAX модель\n",
    "<li> Сохраняем модель (??? Может быть без данных, чтобы сэкономить место).\n",
    "<li> Делаем предсказание\n",
    "<li> Сохраняем предсказение\n",
    "<li> Идём на второй или первый шаг\n",
    "<ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# выберу настроечные ряды руками\n",
    "fitSeries = {'gr18':'1274','gr19':'1684','gr10':'1333','gr11':'1075','gr12':'2118','gr13':'1387','gr14':'1384','gr15':'1174'}\n",
    "fitSeries.update({'gr16':'1483','gr17':'1282','gr21':'1184','gr20':'1131','gr23':'1332','gr22':'1580','gr6':'1177','gr7':'1388'})\n",
    "fitSeries.update({'gr4':'1128','gr2':'1234','gr3':'1231','gr0':'1286','gr1':'1125','gr8':'1181','gr9':'1532'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveResults(df, fName):\n",
    "    \"\"\"\n",
    "    Save dataframe df to file fName\n",
    "    \"\"\"\n",
    "    f = open(fName, 'w')\n",
    "    for ts in df.index.levels[0]:\n",
    "        for lag in df.index.levels[1][6:-5]:\n",
    "            for i in np.arange(6):\n",
    "                try:\n",
    "                    historyStart = lag - datetime.timedelta(hours = 1)\n",
    "                    res = df.loc[ts,lag].y[i]\n",
    "                    if res<0:\n",
    "                        res = 0\n",
    "                    s =  str(ts)+'_'+datetime.datetime.strftime(historyStart, \"%Y-%m-%d\") +'_'+ str(historyStart.hour)+'_'+str(i+1)+','+str(res)+'\\n'\n",
    "                except Exception as ins:\n",
    "                    print lag, ts, i\n",
    "                    print ins\n",
    "                else:\n",
    "                    f.write(s)\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group ID is gr18\n",
      "Regions is  1273\n",
      "Already done!\n",
      "Regions is  1274\n",
      "Already done!\n",
      "Group ID is gr19\n",
      "Regions is  1434\n",
      "Already done!\n",
      "Regions is  1435\n",
      "Already done!\n",
      "Regions is  1437\n",
      "Already done!\n",
      "Regions is  1438\n",
      "Already done!\n",
      "Regions is  1630\n",
      "Already done!\n",
      "Regions is  1684\n",
      "Already done!\n",
      "Group ID is gr10\n",
      "Regions is  1333\n",
      "Already done!\n",
      "Regions is  1337\n",
      "Already done!\n",
      "Regions is  1338\n",
      "Already done!\n",
      "Regions is  1339\n",
      "Already done!\n",
      "Regions is  1783\n",
      "Already done!\n",
      "Group ID is gr11\n",
      "Regions is  1075\n",
      "Already done!\n",
      "Regions is  1733\n",
      "Already done!\n",
      "Regions is  1734\n",
      "Already done!\n",
      "Group ID is gr12\n",
      "Regions is  2069\n",
      "Already done!\n",
      "Regions is  2118\n",
      "Already done!\n",
      "Group ID is gr13\n",
      "Regions is  1222\n",
      "Already done!\n",
      "Regions is  1223\n",
      "Already done!\n",
      "Regions is  1224\n",
      "Already done!\n",
      "Regions is  1225\n",
      "Already done!\n",
      "Regions is  1227\n",
      "Already done!\n",
      "Regions is  1385\n",
      "Already done!\n",
      "Regions is  1386\n",
      "Already done!\n",
      "Regions is  1387\n",
      "Already done!\n",
      "Regions is  1390\n",
      "Already done!\n",
      "Group ID is gr14\n",
      "Regions is  1380\n",
      "Already done!\n",
      "Regions is  1382\n",
      "Already done!\n",
      "Regions is  1383\n",
      "Already done!\n",
      "Regions is  1384\n",
      "Already done!\n",
      "Group ID is gr15\n",
      "Regions is  1132\n",
      "Already done!\n",
      "Regions is  1172\n",
      "Already done!\n",
      "Regions is  1173\n",
      "Already done!\n",
      "Regions is  1174\n",
      "Already done!\n",
      "Group ID is gr16\n",
      "Regions is  1480\n",
      "Already done!\n",
      "Regions is  1482\n",
      "Already done!\n",
      "Regions is  1483\n",
      "Already done!\n",
      "Regions is  1533\n",
      "Already done!\n",
      "Group ID is gr17\n",
      "Regions is  1278\n",
      "Already done!\n",
      "Regions is  1279\n",
      "Already done!\n",
      "Regions is  1281\n",
      "Already done!\n",
      "Regions is  1282\n",
      "Already done!\n",
      "Regions is  2068\n",
      "Already done!\n",
      "Regions is  2119\n",
      "Already done!\n",
      "Regions is  2168\n",
      "Already done!\n",
      "Group ID is gr21\n",
      "Regions is  1184\n",
      "Already done!\n",
      "Regions is  1221\n",
      "Already done!\n",
      "Regions is  1228\n",
      "Already done!\n",
      "Group ID is gr20\n",
      "Regions is  1131\n",
      "Already done!\n",
      "Regions is  1272\n",
      "Already done!\n",
      "Group ID is gr23\n",
      "Regions is  1331\n",
      "Already done!\n",
      "Regions is  1332\n",
      "Already done!\n",
      "Regions is  1376\n",
      "Already done!\n",
      "Regions is  1377\n",
      "Already done!\n",
      "Regions is  1378\n",
      "Already done!\n",
      "Regions is  1431\n",
      "Already done!\n",
      "Group ID is gr22\n",
      "Regions is  1327\n",
      "Already done!\n",
      "Regions is  1580\n",
      "Already done!\n",
      "Group ID is gr6\n",
      "Regions is  1177\n",
      "Already done!\n",
      "Regions is  1326\n",
      "Already done!\n",
      "Regions is  1426\n",
      "Already done!\n",
      "Regions is  1176\n",
      "Already done!\n",
      "Group ID is gr7\n",
      "Regions is  1388\n",
      "Already done!\n",
      "Regions is  1389\n",
      "Already done!\n",
      "Group ID is gr4\n",
      "Regions is  1127\n",
      "Already done!\n",
      "Regions is  1128\n",
      "Already done!\n",
      "Regions is  1129\n",
      "Already done!\n",
      "Regions is  1130\n",
      "Already done!\n",
      "Group ID is gr2\n",
      "Regions is  1180\n",
      "Already done!\n",
      "Regions is  1232\n",
      "Already done!\n",
      "Regions is  1233\n",
      "Already done!\n",
      "Regions is  1234\n",
      "Already done!\n",
      "Regions is  1235\n",
      "Already done!\n",
      "Group ID is gr3\n",
      "Regions is  1183\n",
      "Already done!\n",
      "Regions is  1229\n",
      "Already done!\n",
      "Regions is  1230\n",
      "Already done!\n",
      "Regions is  1231\n",
      "Already done!\n",
      "Regions is  1436\n",
      "Already done!\n",
      "Group ID is gr0\n",
      "Regions is  1283\n",
      "Already done!\n",
      "Regions is  1284\n",
      "Already done!\n",
      "Regions is  1285\n",
      "Already done!\n",
      "Regions is  1286\n",
      "Already done!\n",
      "Regions is  1287\n",
      "Already done!\n",
      "Group ID is gr1\n",
      "Regions is  1076\n",
      "Already done!\n",
      "Regions is  1077\n",
      "Already done!\n",
      "Regions is  1125\n",
      "Already done!\n",
      "Regions is  1126\n",
      "Already done!\n",
      "Group ID is gr8\n",
      "Regions is  1175\n",
      "Already done!\n",
      "Regions is  1178\n",
      "Already done!\n",
      "Regions is  1179\n",
      "Already done!\n",
      "Regions is  1181\n",
      "Already done!\n",
      "Regions is  1182\n",
      "Already done!\n",
      "Regions is  1280\n",
      "Already done!\n",
      "Regions is  1334\n",
      "Already done!\n",
      "Regions is  1335\n",
      "Already done!\n",
      "Regions is  1336\n",
      "Already done!\n",
      "Regions is  1439\n",
      "Already done!\n",
      "Regions is  1441\n",
      "Already done!\n",
      "Regions is  1442\n",
      "Already done!\n",
      "Group ID is gr9\n",
      "Regions is  1530\n",
      "Already done!\n",
      "Regions is  1532\n",
      "Already done!\n",
      "Total error is 0\n"
     ]
    }
   ],
   "source": [
    "# диапазон дат для обучения\n",
    "startFit = '2015-01-01 0:0:0'\n",
    "endFit = '2016-05-31 23:00:00'\n",
    "\n",
    "err = 0\n",
    "\n",
    "# диапазон дат для предсказания\n",
    "startPrediction = '2016-05-31 18:00:00'\n",
    "endPrediction   = '2016-06-30 23:00:00'\n",
    "predictionRange = pd.date_range(startPrediction, endPrediction, freq='H')\n",
    "\n",
    "# словарь с оптимальными параметрами для каждой группы\n",
    "#paramsGroups = np.load('paramsGroups.npy').item()\n",
    "\n",
    "# create array to save prediction results\n",
    "mIndex = pd.MultiIndex.from_product([df.columns.values, predictionRange])\n",
    "#resDf = pd.DataFrame(index = mIndex, columns = ['y','err'])\n",
    "# load data\n",
    "resDf = pd.read_pickle('predictionResults7.pcl')\n",
    "#recalcRegions = [1272,1377]\n",
    "\n",
    "dp = 0\n",
    "dq = 0\n",
    "dp2 = 0\n",
    "dq2 = 0\n",
    "\n",
    "for grId, ts in tsGroups.iteritems(): #{k: v for k, v in tsGroups.iteritems() if k in recalcGroups}.iteritems():\n",
    "    \n",
    "    print 'Group ID is', grId\n",
    "    \n",
    "    # получаем параметры SARIMAX модели\n",
    "    params = paramsGroups.get(grId)[1] \n",
    "        \n",
    "    for tsId in ts:\n",
    "        \n",
    "        print 'Regions is ', tsId\n",
    "        # получаем временной ряд\n",
    "        ts = df.loc[startFit:endFit,tsId] #\n",
    "        \n",
    "        if ~np.isnan(resDf.loc[tsId,'2016-06-15'].err):\n",
    "            print 'Already done!'\n",
    "            continue\n",
    "\n",
    "        # обучаем регрессор\n",
    "        ts = ts.to_frame(name = 'trip_count')\n",
    "        \n",
    "        \n",
    "        [best_aic,best_params, best_model] = findHyperParams(ts)\n",
    "        \n",
    "        print best_params\n",
    "        params = best_params\n",
    "        \n",
    "        [r_pr, res, regressor] = regression(addFeatures(ts), verbose = True, searchBestFit = True)\n",
    "        exog = getRegressor(regressor,startFit,endFit)\n",
    "        \n",
    "        # обучаем SARIMAX модель\n",
    "        print 'Teach SARIMAX'\n",
    "        try:\n",
    "            mSARIMA = best_model\n",
    "            #mSARIMA=sm.tsa.statespace.SARIMAX(ts, order=[params[0]+dp, 2, params[1]+dq],\n",
    "            #                                  seasonal_order=(params[2]+dp2, 1, params[3]+dq2, 24),\n",
    "            #                                  exog = exog).fit(disp=1);\n",
    "            \n",
    "            # получаем предсказания регрессора на весь диапазон дат (обучение+предсказание)\n",
    "            exog = getRegressor(regressor,startFit,endPrediction)\n",
    "            # получаем данные о поездкахы на весь диапазон дат\n",
    "            endog = df.loc[startFit:endPrediction,tsId]\n",
    "       \n",
    "            # создаём новую модель, которую будет использовать для предсказания\n",
    "            model_fitted = sm.tsa.statespace.SARIMAX(endog, order=[params[0]+dp, 2, params[1]+dq],\n",
    "                                                 seasonal_order=(params[2]+dp2, 1, params[3]+dq2, 24),\n",
    "                                                 exog = exog).filter(mSARIMA.params)\n",
    "        except Exception as inst:\n",
    "            print 'Can not teach or create the model'\n",
    "            print inst\n",
    "            continue\n",
    "        else:    \n",
    "            # проходим по всему диапазону дат предсказаний\n",
    "            print 'Make prediction'\n",
    "            for firstLag in predictionRange:\n",
    "                lastLag = firstLag+datetime.timedelta(hours = 5)\n",
    "                # prediction\n",
    "                try:\n",
    "                    predicted_data = model_fitted.predict(firstLag, lastLag, dynamic=True, exog = exog[firstLag:lastLag])\n",
    "                except Exception as inst:\n",
    "                    print 'Prediction error'\n",
    "                    print inst\n",
    "                else:\n",
    "                    # save results\n",
    "                    resDf.loc[tsId,firstLag].y = predicted_data\n",
    "                    err += (df.loc[startPrediction:endPrediction,tsId]-predicted_data).abs().sum()\n",
    "                    resDf.loc[tsId,firstLag].err = (df.loc[startPrediction:endPrediction,tsId]-predicted_data).abs().mean()\n",
    "\n",
    "            # save results\n",
    "            resDf.to_pickle('predictionResults8.pcl')\n",
    "    \n",
    "print 'Total error is', err    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resDf.to_pickle('predictionResults8.pcl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-01 16:00:00 1175 0\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-01 16:00:00 1175 1\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-01 16:00:00 1175 2\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-01 16:00:00 1175 3\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-01 16:00:00 1175 4\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-01 16:00:00 1175 5\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-01 17:00:00 1175 0\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-01 17:00:00 1175 1\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-01 17:00:00 1175 2\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-01 17:00:00 1175 3\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-01 17:00:00 1175 4\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-01 17:00:00 1175 5\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 20:00:00 1442 0\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 20:00:00 1442 1\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 20:00:00 1442 2\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 20:00:00 1442 3\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 20:00:00 1442 4\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 20:00:00 1442 5\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 21:00:00 1442 0\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 21:00:00 1442 1\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 21:00:00 1442 2\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 21:00:00 1442 3\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 21:00:00 1442 4\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 21:00:00 1442 5\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 22:00:00 1442 0\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 22:00:00 1442 1\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 22:00:00 1442 2\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 22:00:00 1442 3\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 22:00:00 1442 4\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 22:00:00 1442 5\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 23:00:00 1442 0\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 23:00:00 1442 1\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 23:00:00 1442 2\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 23:00:00 1442 3\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 23:00:00 1442 4\n",
      "'float' object has no attribute '__getitem__'\n",
      "2016-06-11 23:00:00 1442 5\n",
      "'float' object has no attribute '__getitem__'\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "#resDf = pd.read_pickle('predictionResults6.pcl')\n",
    "saveResults(resDf,'m11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=23.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
