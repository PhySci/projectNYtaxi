{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем делать это следующим образом:\n",
    "<ol> План работы\n",
    "<li> Регрессор по тем же параметрам, что использовал в третьей неделе.\n",
    "<li> Предсказания на 6 часов вперёд по SARIMAX\n",
    "<li> Решающий лес\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frodos/anaconda/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn import preprocessing, linear_model, model_selection\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "import statsmodels.api as sm\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import make_scorer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processDataFrame(inpDf, Kw = 7, Ka = 2):\n",
    "    \"\"\"\n",
    "    Обрабатываем сразу весь dateFrame и добавляем признаки, общие для всех рядов\n",
    "    тренд, гармоники, категориальные перемнные\n",
    "    для дат, дней недели, etc)\n",
    "\n",
    "    Parameters:\n",
    "    Kw number of weeks harmonics\n",
    "    Ka number of annual harmonics\n",
    "    \"\"\"   \n",
    "    inpDf = inpDf.assign(linear = (inpDf.index - datetime.datetime(2014,1,1,0,0,0))/np.timedelta64(1, 'h'))\n",
    "    \n",
    "    # час — эти признаки можно пробовать брать и категориальными\n",
    "    # и непрерывными, можно даже и так, и так\n",
    "\n",
    "    # добавляем гармонические фичи\n",
    "    for ind in range(1,Kw+1):\n",
    "        inpDf['weekCos'+str(ind)]= np.cos(np.pi*inpDf.linear*ind/168)\n",
    "        inpDf['weekSin'+str(ind)]= np.sin(np.pi*inpDf.linear*ind/168)\n",
    "     \n",
    "    for ind in range(1,Ka+1):\n",
    "        inpDf['yearCos'+str(ind)]= np.cos(2*np.pi*inpDf.linear*ind/8766)        \n",
    "        inpDf['yearSin'+str(ind)]= np.sin(2*np.pi*inpDf.linear*ind/8766)\n",
    "\n",
    "    # добавляем числовое и категориальные свойства для дней недели\n",
    "    inpDf = inpDf.assign(dayOfWeek = inpDf.index.dayofweek)\n",
    "    lbDays = preprocessing.LabelBinarizer()\n",
    "    lbDays.fit(list(np.arange(6)))\n",
    "    DoW = pd.DataFrame(lbDays.transform(inpDf.index.dayofweek),columns = ['dayOfWeek_'+str(x) for x in np.arange(6)],\n",
    "                       index = inpDf.index)      \n",
    "    inpDf = inpDf.merge(DoW,left_index=True,right_index=True)\n",
    "\n",
    "    # добавляем dummy variables для месяца\n",
    "    inpDf = inpDf.assign(month = inpDf.index.month)\n",
    "    lbMonths = preprocessing.LabelBinarizer()\n",
    "    lbMonths.fit(list(np.arange(12)))\n",
    "    Months = pd.DataFrame(lbMonths.transform(inpDf.index.month),columns = ['month_'+str(x) for x in np.arange(1,13)],\n",
    "                          index = inpDf.index)      \n",
    "    inpDf = inpDf.merge(Months,left_index=True,right_index=True);\n",
    "\n",
    "    # добавляем год (вещественный)\n",
    "    inpDf = inpDf.assign(year = inpDf.index.year)\n",
    "\n",
    "    # добавляем день месяца (вещественный)\n",
    "    inpDf = inpDf.assign(day = inpDf.index.day)\n",
    "\n",
    "    # добавляем час (вещественный и категориальный)\n",
    "    inpDf = inpDf.assign(hour = inpDf.index.hour)\n",
    "    lbHours = preprocessing.LabelBinarizer()\n",
    "    lbHours.fit(list(np.arange(24)))\n",
    "    Hours = pd.DataFrame(lbHours.transform(inpDf.index.hour),columns = ['hour_'+str(x) for x in np.arange(24)],\n",
    "                       index = inpDf.index)      \n",
    "    inpDf = inpDf.merge(Hours,left_index=True,right_index=True)\n",
    "    \n",
    "    return inpDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saveFile(df_inp,fName = 'w5.csv'):\n",
    "    f = open(fName,'w')\n",
    "    f.writelines('id,y\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "    df = df_inp.reset_index()\n",
    "    df.rename(columns={'index':'date'},inplace=True)\n",
    "\n",
    "    for ind, row in df.iterrows():\n",
    "        historyStart = row.date\n",
    "\n",
    "        if historyStart > datetime.datetime(2016,6,30,17):\n",
    "            continue\n",
    "\n",
    "        s0 = str(row.region)+'_'+ str(datetime.datetime.strftime(historyStart, \"%Y-%m-%d\"))+ '_'+ str(historyStart.hour)\n",
    "\n",
    "        s1 = s0 +'_1,'+str(rnd(row.get('y1'))) + '\\n'\n",
    "        f.writelines(s1)\n",
    "\n",
    "        s2 = s0 +'_2,'+str(rnd(row.get('y2'))) + '\\n'\n",
    "        f.writelines(s2)\n",
    "\n",
    "        s3 = s0 +'_3,'+str(rnd(row.get('y3'))) + '\\n'\n",
    "        f.writelines(s3)\n",
    "\n",
    "        s4 = s0 +'_4,'+str(rnd(row.get('y4'))) + '\\n'\n",
    "        f.writelines(s4)\n",
    "\n",
    "        s5 = s0 +'_5,'+str(rnd(row.get('y5'))) + '\\n'\n",
    "        f.writelines(s5)\n",
    "\n",
    "        s6 = s0 +'_6,'+str(rnd(row.get('y6'))) + '\\n'\n",
    "        f.writelines(s6)\n",
    "\n",
    "    f.close()    \n",
    "    \n",
    "def rnd(x):\n",
    "    return int(np.round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define time frames\n",
    "startTrain = '2015-01-01 00:00:00'\n",
    "endTrain   = '2016-04-30 23:00:00'\n",
    "\n",
    "startValidation = '2016-05-01 00:00:00'\n",
    "endValidation   = '2016-05-31 23:00:00'\n",
    "\n",
    "startTest = '2016-06-01 00:00:00'\n",
    "endTest   = '2016-06-30 23:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# id нужных регионов\n",
    "regsDf = pd.read_csv('../crowdRegs.csv',names=['id','regId']);  \n",
    "\n",
    "# времянные ряды для этих регионов\n",
    "df = pd.read_pickle('../loadData/crowdRegs3.pcl')\n",
    "regNames = regsDf.regId.values.astype('str')\n",
    "df.columns = regNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# общая обработка данных\n",
    "tmp = df.columns.values\n",
    "df = processDataFrame(df,Kw = 7, Ka = 4)\n",
    "commonFeatures =  list(set(df.columns)-set(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to flat structure\n",
    "df2 = pd.DataFrame()\n",
    "for regName in regNames:\n",
    "    tDf = df.loc[:,regName.split() + commonFeatures].rename(columns={regName:'y'})\n",
    "    tDf = tDf.assign(region = regName)\n",
    "    df2 = pd.concat([df2,tDf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем улучшить качество регрессора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE is 35.148706584\n"
     ]
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "linReg = linear_model.Ridge(alpha=100)\n",
    "#ppl = Pipeline([('poly', poly), ('Ridge', linReg)])\n",
    "ppl = Pipeline([('Ridge', linReg)])\n",
    "\n",
    "# делает регрессию на признаки\n",
    "df2 = df2.assign(regressor = 0)\n",
    "for regName in regNames:\n",
    "    subSet = df2.query('region == @regName')\n",
    "    \n",
    "    ppl.fit(subSet.loc[startTrain:endValidation,:].drop(['y','regressor'],axis=1),\n",
    "               subSet.loc[startTrain:endValidation,'y'])\n",
    "    prediction = ppl.predict(subSet.drop(['y','regressor'],axis=1))\n",
    "    prediction[prediction<0]=0\n",
    "    df2.loc[df2.region == regName,'regressor'] = prediction\n",
    "    \n",
    "print 'MAE is', MAE(df2.regressor,df2.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скор на лидерборде 47.52875 \n",
    " Ничего абсолютно не понимаю. Как так может быть???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df2.assign(sarimax1 = 0).assign(sarimax2 = 0).assign(sarimax3 = 0)\n",
    "df2 = df2.assign(sarimax4 = 0).assign(sarimax5 = 0).assign(sarimax6 = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.to_pickle('24July.pcl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle('24July.pcl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df2.loc[startTrain:endValidation,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn SARIMAX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frodos/anaconda/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can not create the model"
     ]
    }
   ],
   "source": [
    "sarimaxParams = np.load('sarimaxParams.npy').item()\n",
    "\n",
    "for regName in regNames:\n",
    "    print 'Learn SARIMAX'\n",
    "    subSet = df2.query('region == @regName')\n",
    "    \n",
    "    # получить параметры SARIMAX моделе\n",
    "    params = sarimaxParams.get(regName)\n",
    "    try: \n",
    "        mSARIMA=sm.tsa.statespace.SARIMAX(subSet.loc[startTrain:endValidation,'y'],\n",
    "                                          order=[params[0], 1, params[1]],\n",
    "                                          seasonal_order=(params[2], 1, params[3], 24),\n",
    "                                          exog = subSet.loc[startTrain:endValidation,'regressor'],\n",
    "                                          enforce_invertibility = True).fit(disp=1);\n",
    "    except Exception as inst:\n",
    "        print type(inst)     \n",
    "        print inst          \n",
    "\n",
    "    # создаём новую модель, которую будет использовать для предсказания\n",
    "    # Для чего такой финт ушами - не понимаю до сих пор\n",
    "    try:\n",
    "        model_fitted = []\n",
    "        model_fitted = sm.tsa.statespace.SARIMAX(subSet.loc[startTrain:endTest,'y'],\n",
    "                                             order=[params[0], 1, params[1]],\n",
    "                                             seasonal_order=(params[2], 1, params[3], 24),\n",
    "                                             exog = subSet.loc[startTrain:endTest,'regressor'],\n",
    "                                             enforce_invertibility = True).filter(mSARIMA.params)\n",
    "        del mSARIMA\n",
    "    except Exception as inst:\n",
    "        print 'Can not create the model'\n",
    "        print inst\n",
    "        continue\n",
    "    else:     \n",
    "        # проходим по всему диапазону дат предсказаний\n",
    "        print 'Make prediction'\n",
    "        for firstLag in subSet.index[:-5]:\n",
    "            lastLag = firstLag+datetime.timedelta(hours = 5)\n",
    "            # prediction\n",
    "            try:\n",
    "                predicted_data = model_fitted.predict(firstLag, lastLag, dynamic=True, exog = subSet.loc[firstLag:lastLag])\n",
    "\n",
    "                subSet.loc[firstLag,'sarimax1'] = predicted_data[0]\n",
    "                subSet.loc[firstLag,'sarimax2'] = predicted_data[1]\n",
    "                subSet.loc[firstLag,'sarimax3'] = predicted_data[2]\n",
    "                subSet.loc[firstLag,'sarimax4'] = predicted_data[3]\n",
    "                subSet.loc[firstLag,'sarimax5'] = predicted_data[4]\n",
    "                subSet.loc[firstLag,'sarimax6'] = predicted_data[5]\n",
    "\n",
    "            except Exception as inst:\n",
    "                print 'Prediction error'\n",
    "                print inst\n",
    "\n",
    "        df2.loc[df2.region == regName,:] = subSet\n",
    "        df2.to_pickle('newSet.pcl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
